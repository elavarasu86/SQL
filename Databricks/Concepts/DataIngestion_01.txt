Data Ingestion From CloudStorage:

 Throught data ingestion large volumne of data stored in organization cloud storage will be covernverted into Delta tables.
 
For example:

CloudStorage			DataIngestion
________			 ___________________			 ___________________
CSV		|			|CREATE TABLE AS	|           |					|
JSON	|			|COPY INTO			|           |                   |
PARQUET	|	=>		|AUTO LOADER		|	=>      |      DeltaTables  |
ORC		|			|					|           |                   |
________|			|___________________|           |___________________|

Three primary methods for ingesting files from cloud object storage are below
	
	1. CREATE TABLE AS (CTAS)
	2. COPY INTO 
	3. AUTO LOADER


Method1: CTAS
	
		CREATE TABLE new_table AS							
		SELECT * 
		FROM read_files(
			<path_to_file(s)>,
			format => '<file_type>',
			<other_format_specific_options)
		);
		
		CREATE TABLE AS (CTAS) creates a Delta table by default from files in cloud object storage
		
		The read_files() function reads files under a provided location and retuns the data in tabular format
			Supports reading file format like JSON, CSV, XML, TEXT, BINARYFILE, PARQUET, AVRO file formats.
			Can detect the file format automatically and infer a unified schema across all files.
			Allows  you to specify format-specific options for greater control when reading source files.
			Can be used in streaming tables to incrementally ingest files into Delta lake using Auto Loader.
			
Method2:	INCREMENTAL BATCH - COPY INTO (legacy)

		CREATE TABLE new_table; 
		
		COPTY INTO new_table
		FROM '<directory_Path>'
		FILEFORMAT = <file_format>
		FORMAT_OPTIONS(<options>)
		COPY_OPTIONS(<options>)

		CREATE TABLE new_table; 
			1. Create an empty table to copy data into. In the above statement schema is not defined. Also we have option to create table with schema.
		
		/* NEED TO CHECK BELOW LINE*/
		This command peforms bulk load.

		COPTY INTO new_table
		FROM '<directory_Path>'
		FILEFORMAT = <file_format>
		FORMAT_OPTIONS(<options>)
		COPY_OPTIONS(<options>)
		
			2. COPY INTO for incremental batch ingestion
			
				- Ia a retriable and idempotent operation and will skip files that have already been loaded(incremental)
				- Support various files types
				- From specifies Cloud Storage Location
				- FORMAT_OPTIONS() helps parse and interprete. 
				- COPY_OPTIONS() helps in determine schema evolution(mergeSchema) and idempotency(force)
				
Method3: Incremental Batch or Streaming - Auto Loader 

		- Incrementally and efficiently processes new data files (in batch or streaming) as they arrive in cloud storage without any additional setup
		- Auto loader has support for both Python and SQL(Leveraging Declarative Pipelines)
		- you can use Auto Loader to process billions of files
		- Auto Loader is built upon Spark Structured Streaming
		
	Python Auto Loader:
	
		(Spark
		.readStream
			.format("cloudFiles")
			.option("cloudFiles.format","json")
			.option("cloudFiles.schemaLocation", "<checkpoint_path>")
			.load("/Volumes/catalog/schema/files")
		.writeStream
			.option("checkpointLocation", "<chekpoint_path>")
			.trigger(processingTime="5 seconds")
			.toTable("catalog.database.table")
		)
		
		Auto Loader in Python to read straming data from cloud storage:
		
			- We start with .readStream and set the format to "cloudFiles", which enables Auto Loader.
			- Then, we specify the file format.
			- cloudFiles.schemaLocation is used to define schema location which is used to track schema inference and evolution.
			
			- checkpointLocation used to maintain state and progress
			- processingTime used to trigger on a specified interval.
	
	SQL Auto Loader:
	
		CREATE OR REFRESH STRAMING TABLE
		catalog.schema.table SCHEDULE every 1 hour
		AS
		SELECT * 
		FROM STREAM read_files(
			'<dir_path>',
			format => '<file_type>'
		)
		
			- Databricks recommends using streaming tables to ingest data with Databricks SQL(instead of COPY INTO). A streaming table is a table registered to Unity Catalog that includes additional support for streaming or incremental data processing. When you create a streaming table, a pipeline is automatically generated for it. Streaming tables can be used for incremental data loading from both Kafka and cloud object storage.
			
			- To create a streaming table from files in a volume, you use Auto Loader. Databricks recommends using Auto Loader with Lakeflow Declarative Pipelines for most data ingestion tasks from cloud object storage. Together, Auto Loader and Declarative Pipelines are designed to incrementally and idempotentaly load continuously growing datasets as they arrive.
			
			- Streamikng tables in Databricks SQL are bakced by serverless Lakeflow Declarative pipelines. Your workspace must support serverless piplines to use this functinality. Alternatively, you can build you own lakeflow Declarative Pipelines for incremental processing, optimization, and monitoring. Declarative Pipelines offer a range of additional features, which you can learn more about here: https://docs.databricks.com/aws/en/dlt/
			
			- To use Auto Loader in Databricks SQL, use the read_files function with the STREAM keyword in the FROM clause.